{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\naufa\\desktop\\top of desk\\git\\cluster-repo\\.venv\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\naufa\\desktop\\top of desk\\git\\cluster-repo\\.venv\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\naufa\\desktop\\top of desk\\git\\cluster-repo\\.venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\naufa\\desktop\\top of desk\\git\\cluster-repo\\.venv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\naufa\\desktop\\top of desk\\git\\cluster-repo\\.venv\\lib\\site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\naufa\\desktop\\top of desk\\git\\cluster-repo\\.venv\\lib\\site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\naufa\\desktop\\top of desk\\git\\cluster-repo\\.venv\\lib\\site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\naufa\\desktop\\top of desk\\git\\cluster-repo\\.venv\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\naufa\\desktop\\top of desk\\git\\cluster-repo\\.venv\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\naufa\\desktop\\top of desk\\git\\cluster-repo\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.21.0-cp312-cp312-win_amd64.whl.metadata (6.3 kB)\n",
      "Collecting numpy (from torchvision)\n",
      "  Downloading numpy-2.2.3-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: torch==2.6.0 in c:\\users\\naufa\\desktop\\top of desk\\git\\cluster-repo\\.venv\\lib\\site-packages (from torchvision) (2.6.0)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
      "  Using cached pillow-11.1.0-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\naufa\\desktop\\top of desk\\git\\cluster-repo\\.venv\\lib\\site-packages (from torch==2.6.0->torchvision) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\naufa\\desktop\\top of desk\\git\\cluster-repo\\.venv\\lib\\site-packages (from torch==2.6.0->torchvision) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\naufa\\desktop\\top of desk\\git\\cluster-repo\\.venv\\lib\\site-packages (from torch==2.6.0->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\naufa\\desktop\\top of desk\\git\\cluster-repo\\.venv\\lib\\site-packages (from torch==2.6.0->torchvision) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\naufa\\desktop\\top of desk\\git\\cluster-repo\\.venv\\lib\\site-packages (from torch==2.6.0->torchvision) (2025.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\naufa\\desktop\\top of desk\\git\\cluster-repo\\.venv\\lib\\site-packages (from torch==2.6.0->torchvision) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\naufa\\desktop\\top of desk\\git\\cluster-repo\\.venv\\lib\\site-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\naufa\\desktop\\top of desk\\git\\cluster-repo\\.venv\\lib\\site-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\naufa\\desktop\\top of desk\\git\\cluster-repo\\.venv\\lib\\site-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
      "Using cached torchvision-0.21.0-cp312-cp312-win_amd64.whl (1.6 MB)\n",
      "Using cached pillow-11.1.0-cp312-cp312-win_amd64.whl (2.6 MB)\n",
      "Downloading numpy-2.2.3-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 4.7/12.6 MB 23.8 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.8/12.6 MB 14.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 9.4/12.6 MB 16.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 9.4/12.6 MB 16.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 9.4/12.6 MB 16.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 9.4/12.6 MB 16.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 10.0/12.6 MB 6.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.7/12.6 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.6/12.6 MB 7.0 MB/s eta 0:00:00\n",
      "Installing collected packages: pillow, numpy, torchvision\n",
      "Successfully installed numpy-2.2.3 pillow-11.1.0 torchvision-0.21.0\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\naufa\\desktop\\top of desk\\git\\cluster-repo\\.venv\\lib\\site-packages (from pandas) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\naufa\\desktop\\top of desk\\git\\cluster-repo\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\naufa\\desktop\\top of desk\\git\\cluster-repo\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "Using cached pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "Using cached tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 pytz-2025.1 tzdata-2025.1\n",
      "Requirement already satisfied: numpy in c:\\users\\naufa\\desktop\\top of desk\\git\\cluster-repo\\.venv\\lib\\site-packages (2.2.3)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch\n",
    "!pip3 install torchvision\n",
    "!pip3 install pandas\n",
    "!pip3 install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T12:11:27.393215Z",
     "iopub.status.busy": "2025-01-25T12:11:27.392842Z",
     "iopub.status.idle": "2025-01-25T12:11:34.395611Z",
     "shell.execute_reply": "2025-01-25T12:11:34.394652Z",
     "shell.execute_reply.started": "2025-01-25T12:11:27.393184Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T12:12:45.916742Z",
     "iopub.status.busy": "2025-01-25T12:12:45.916361Z",
     "iopub.status.idle": "2025-01-25T12:12:47.271882Z",
     "shell.execute_reply": "2025-01-25T12:12:47.270804Z",
     "shell.execute_reply.started": "2025-01-25T12:12:45.916697Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# load the training-set\n",
    "batch_size = 64\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root='../cs-424-ass-1-wednesday-class/train', transform=transform_train)\n",
    "\n",
    "# Split dataset into training and validation sets (e.g., 80% train, 20% validation)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T12:14:39.555389Z",
     "iopub.status.busy": "2025-01-25T12:14:39.554938Z",
     "iopub.status.idle": "2025-01-25T12:14:39.559951Z",
     "shell.execute_reply": "2025-01-25T12:14:39.558602Z",
     "shell.execute_reply.started": "2025-01-25T12:14:39.555339Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T14:02:49.174099Z",
     "iopub.status.busy": "2025-01-25T14:02:49.173781Z",
     "iopub.status.idle": "2025-01-25T14:02:49.360579Z",
     "shell.execute_reply": "2025-01-25T14:02:49.359635Z",
     "shell.execute_reply.started": "2025-01-25T14:02:49.174074Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define ResNet-18 \n",
    "class CustomResNet18(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CustomResNet18, self).__init__()\n",
    "\n",
    "        self.model = models.resnet18(weights=None)\n",
    "        self.model.fc = nn.Sequential(\n",
    "            nn.Linear(self.model.fc.in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "\n",
    "num_classes = 10   \n",
    "model = CustomResNet18(num_classes=num_classes).to(dev)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "learning_rate = 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T14:02:54.038041Z",
     "iopub.status.busy": "2025-01-25T14:02:54.037707Z",
     "iopub.status.idle": "2025-01-25T14:02:54.043854Z",
     "shell.execute_reply": "2025-01-25T14:02:54.042711Z",
     "shell.execute_reply.started": "2025-01-25T14:02:54.038010Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# OneCycleLR Scheduler\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.002, epochs=100, steps_per_epoch=len(train_loader), pct_start=0.3)\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "class Cutout(object):\n",
    "    def __init__(self, n_holes, length):\n",
    "        self.n_holes = n_holes\n",
    "        self.length = length\n",
    "\n",
    "    def __call__(self, img):\n",
    "        w, h = img.size\n",
    "        mask = np.ones((h, w), np.float32)\n",
    "\n",
    "        for n in range(self.n_holes):\n",
    "            y = np.random.randint(h)\n",
    "            x = np.random.randint(w)\n",
    "            y1 = np.clip(y - self.length // 2, 0, h)\n",
    "            y2 = np.clip(y + self.length // 2, 0, h)\n",
    "            x1 = np.clip(x - self.length // 2, 0, w)\n",
    "            x2 = np.clip(x + self.length // 2, 0, w)\n",
    "\n",
    "            mask[y1:y2, x1:x2] = 0.\n",
    "\n",
    "        mask = Image.fromarray(mask * 255).convert('L')\n",
    "        img = Image.composite(img, Image.new('RGB', img.size), mask)\n",
    "        return img\n",
    "\n",
    "\n",
    "# EarlyStopping class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0):\n",
    "        self.patience = patience  # Number of epochs to wait for improvement\n",
    "        self.delta = delta  # Minimum change to qualify as an improvement\n",
    "        self.counter = 0  # Counter for epochs with no improvement\n",
    "        self.best_score = None  # Best score (validation loss or accuracy)\n",
    "        self.early_stop = False  # Flag to indicate if early stopping should occur\n",
    "        self.best_model_wts = None  # Best model weights\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss  # We use negative loss since we want to minimize the loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_model_wts = model.state_dict()\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.best_model_wts = model.state_dict()\n",
    "            self.counter = 0\n",
    "            print(\"Saved New Best Weight!\")\n",
    "\n",
    "# Function for random augmentations that change each epoch\n",
    "def get_random_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(40),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.4, saturation=0.4, hue=0.2),\n",
    "        transforms.RandomAffine(degrees=15, translate=(0.1, 0.1)),\n",
    "        transforms.RandomPerspective(distortion_scale=0.3, p=0.5, interpolation=3),\n",
    "        transforms.RandomCrop(224, padding=4),  # Added RandomCrop with padding\n",
    "        transforms.RandomAffine(degrees=0, scale=(0.9, 1.1)),  # Added random zoom\n",
    "        transforms.GaussianBlur(kernel_size=5),  # Added Gaussian blur\n",
    "        Cutout(n_holes=1, length=32),  # Added Cutout with 1 hole of length 32\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "\n",
    "# Mixup function implementation\n",
    "def mixup_data(x, y, alpha=0.3):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1.0\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).cuda()\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "early_stopping = EarlyStopping(patience=30, delta=0.001)\n",
    "\n",
    "# Modified train function with mixup and random augmentations\n",
    "def train_model():\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        # Reinitialize augmentations for each epoch\n",
    "        transform_train = get_random_transform()\n",
    "        train_dataset = datasets.ImageFolder(root='../cs-424-ass-1-wednesday-class/train', transform=transform_train)\n",
    "        train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(dev), labels.to(dev)\n",
    "\n",
    "            # Apply Mixup\n",
    "            mixed_images, targets_a, targets_b, lam = mixup_data(images, labels)\n",
    "\n",
    "            outputs = model(mixed_images)\n",
    "            loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_samples += images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_correct += (lam * predicted.eq(targets_a).sum().item() + (1 - lam) * predicted.eq(targets_b).sum().item())\n",
    "        \n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_accuracy = total_correct / total_samples\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(dev), labels.to(dev)\n",
    "                \n",
    "                # Apply augmentations to the validation set\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                total_val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss = total_val_loss / len(val_loader)\n",
    "        val_accuracy = correct_val / total_val\n",
    "\n",
    "        # Print loss and accuracy for both training and \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, \"\n",
    "              f\"Learning Rate: {scheduler.get_lr()[0]:.10f}\")\n",
    "        \n",
    "        early_stopping(train_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered\")\n",
    "            model.load_state_dict(early_stopping.best_model_wts)  # Load the best model weights\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Patience Counter: {early_stopping.counter}/{early_stopping.patience}\")\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), 'resnet18_local.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'if' statement on line 1 (4275098179.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[10], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    num_epochs = 100\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after 'if' statement on line 1\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    num_epochs = 100\n",
    "    train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T14:23:38.763360Z",
     "iopub.status.busy": "2025-01-25T14:23:38.763007Z",
     "iopub.status.idle": "2025-01-25T14:23:38.769210Z",
     "shell.execute_reply": "2025-01-25T14:23:38.768360Z",
     "shell.execute_reply.started": "2025-01-25T14:23:38.763325Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.914133071899414"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model size (should be less than 26)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_params/(1024*1024) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T14:23:42.817535Z",
     "iopub.status.busy": "2025-01-25T14:23:42.817165Z",
     "iopub.status.idle": "2025-01-25T14:23:44.300759Z",
     "shell.execute_reply": "2025-01-25T14:23:44.299640Z",
     "shell.execute_reply.started": "2025-01-25T14:23:42.817502Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# load the test-set\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "test_dataset = datasets.ImageFolder(root='../cs-424-ass-1-wednesday-class/test', transform=transform_test)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T14:23:47.148389Z",
     "iopub.status.busy": "2025-01-25T14:23:47.148056Z",
     "iopub.status.idle": "2025-01-25T14:23:47.153730Z",
     "shell.execute_reply": "2025-01-25T14:23:47.152743Z",
     "shell.execute_reply.started": "2025-01-25T14:23:47.148356Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# test the model\n",
    "def test_model():\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    image_paths = [path for path, _ in test_dataset.imgs]   \n",
    "\n",
    "    old_prefix = \"..\"\n",
    "    new_prefix = \"/kaggle/input\"\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, _ in test_loader:   \n",
    "            images = images.to(dev)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    adjusted_paths= [path.replace(old_prefix, new_prefix) for path in image_paths]\n",
    "    final_paths= [path.replace(\"\\\\\", \"/\") for path in adjusted_paths]\n",
    "\n",
    "    # prediction.csv\n",
    "    df = pd.DataFrame({\n",
    "        'id': final_paths,   \n",
    "        'label': predictions\n",
    "    })\n",
    "    df.to_csv('naufala.2022.csv', index=False)\n",
    "    print(\"Results saved to naufala.2022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T14:24:03.647682Z",
     "iopub.status.busy": "2025-01-25T14:24:03.647335Z",
     "iopub.status.idle": "2025-01-25T14:24:24.477746Z",
     "shell.execute_reply": "2025-01-25T14:24:24.476848Z",
     "shell.execute_reply.started": "2025-01-25T14:24:03.647659Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to naufala.2022.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    test_model()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 10911544,
     "sourceId": 91922,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
